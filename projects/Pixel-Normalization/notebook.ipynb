{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b8c337",
   "metadata": {},
   "source": [
    "# Impact of Pixel Normalization on the Performance of Different CNN Activation Functions in FashionMNIST\n",
    "\n",
    "## 1. Conceptual question\n",
    "Does pixel normalization affect the performance of ReLU and Tanh activation functions in CNNs?\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Hypothesis\n",
    "We hypothesize that Tanh performs better when input pixel values are normalized to a narrow range near zero (e.g., [-1, 1]), whereas ReLU demonstrates superior performance when pixel values span a wider range (e.g., [-5, 5]).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Experimental design\n",
    "In this experiment, a CNN architecture was designed where all structural components remained constant, with the exception of the activation functions. The controlled variables were the range of pixel values and the type of activation function. To test the hypothesis, we compared the loss convergence and accuracy of models employing ReLU versus Tanh across different input ranges. This comparison aims to clearly elucidate the performance disparities between CNNs utilizing different activation functions.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Experiment code\n",
    "Code used to generate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90fd1fc",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "#Set the hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_workers\": 2,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"switch_epoch\": 5,\n",
    "    \"device\": device\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e38690",
   "metadata": {},
   "source": [
    "### 2. Data Loading (Small Range [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baaa8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # normalize the pixel value range to [-1,1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data_project2_DL\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data_project2_DL\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"num_workers\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944f00d",
   "metadata": {},
   "source": [
    "### 3. Data Loading (Large Range [-5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa139560",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_new = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.1,))# normalize the pixel value range to [-5,5]\n",
    "])\n",
    "\n",
    "train_dataset_modified = datasets.FashionMNIST(\n",
    "    root=\"./data_project2_DL\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_new\n",
    ")\n",
    "\n",
    "test_dataset_modified = datasets.FashionMNIST(\n",
    "    root=\"./data_project2_DL\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_new\n",
    ")\n",
    "\n",
    "train_loader_modified = DataLoader(\n",
    "    train_dataset_modified,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "test_loader_modified = DataLoader(\n",
    "    test_dataset_modified,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"num_workers\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29faddb1",
   "metadata": {},
   "source": [
    "### 4. Define the CNN that can use different activation function either Tanh or ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2da148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class different_activation_CNN(nn.Module):\n",
    "    def __init__(self,activation=\"ReLU\"):\n",
    "        super(different_activation_CNN,self).__init__()\n",
    "        self.activation=activation\n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,padding=1)\n",
    "        self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,padding=1)\n",
    "        self.conv3=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv4=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3)\n",
    "        self.conv5=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3)\n",
    "        self.lin1=nn.Linear(in_features=64*3*3,out_features=64)\n",
    "        self.lin2=nn.Linear(in_features=64,out_features=32)\n",
    "        self.lin3=nn.Linear(in_features=32,out_features=10)\n",
    "        self.activation_records = []\n",
    "        self.use_record = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        layers = [\n",
    "            (\"conv1\", self.conv1, True),\n",
    "            (\"conv2\", self.conv2, True),\n",
    "            (\"conv3\", self.conv3, False),\n",
    "            (\"conv4\", self.conv4, False),\n",
    "            (\"conv5\", self.conv5, False),\n",
    "            (\"lin1\", self.lin1, False),\n",
    "            (\"lin2\", self.lin2, False),\n",
    "            (\"lin3\", self.lin3, False),\n",
    "        ]\n",
    "\n",
    "        for name, layer, if_pool in layers:\n",
    "            x = layer(x)\n",
    "\n",
    "            if self.use_record: # If we want to record the pre-activation values\n",
    "                activations = x.detach().cpu().numpy().flatten()\n",
    "                if len(activations) > 5000: # We only select up to 5000 pre-activation-values\n",
    "                    activations = np.random.choice(activations, 5000, replace=False)\n",
    "                self.activation_records.append((name, activations))\n",
    "\n",
    "            if name.startswith(\"lin\") and x.dim() > 2:\n",
    "                x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            if name != \"lin3\":\n",
    "                x = F.relu(x) if self.activation == \"ReLU\" else torch.tanh(x)\n",
    "\n",
    "            if if_pool:\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def switch_activation(self,activation):\n",
    "        self.activation=activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ec74b",
   "metadata": {},
   "source": [
    "### 5. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_activation_train_with_recording(model,train_loader,device,optimizer):\n",
    "    model.train()\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    losses=[]\n",
    "    epoch_activations={f'conv{i}':[] for i in range(1,6)}\n",
    "    epoch_activations.update({f'lin{i}':[] for i in range(1,3)})\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        one_epoch_loss=0\n",
    "        for idx,(img,label) in enumerate(train_loader):\n",
    "            model.use_record=(idx==0)\n",
    "            img=img.to(device)\n",
    "            label=label.to(device)\n",
    "            model.activation_records=[]\n",
    "            output=model(img)\n",
    "            loss=criterion(output,label)\n",
    "            one_epoch_loss+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if idx==0:\n",
    "                for layer_name,values in model.activation_records:\n",
    "                    epoch_activations[layer_name].append(values)\n",
    "                model.activation_records=[]\n",
    "                model.use_record=False\n",
    "        losses.append(one_epoch_loss)\n",
    "    losses=np.array(losses)/len(train_loader)\n",
    "    print(\"Average loss:\",np.mean(losses))\n",
    "    return losses,epoch_activations\n",
    "\n",
    "\n",
    "def single_activation_test(model,test_loader,device):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for img,label in test_loader:\n",
    "            img=img.to(device)\n",
    "            label=label.to(device)\n",
    "            output=model(img)\n",
    "            _,predicted=torch.max(output,1)\n",
    "            total+=label.shape[0]\n",
    "            correct+=((predicted==label).sum().item())\n",
    "    print(\"Accuracy:\",100*correct/total)\n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074f68c",
   "metadata": {},
   "source": [
    "### 5. Visualize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f84c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_violinplots_comparison(epoch_activations1, activation_name1,epoch_activations2, activation_name2,save_dir):\n",
    "    def plot_violin_subplot(ax, data1, data2, epoch_indices, xlabel_suffix, colors):\n",
    "        combined, positions = [], []\n",
    "        for i, epoch_idx in enumerate(epoch_indices):\n",
    "            combined.append(data1[epoch_idx])\n",
    "            combined.append(data2[epoch_idx])\n",
    "            positions.append(i * 3 + 1)\n",
    "            positions.append(i * 3 + 1.8)\n",
    "\n",
    "        parts = ax.violinplot(combined, positions=positions, widths=0.7,\n",
    "                              showmeans=True, showmedians=True)\n",
    "        for i, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[i % 2])\n",
    "            pc.set_alpha(0.7)\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_linewidth(0.5)\n",
    "        for partname in ('cbars','cmins','cmaxes','cmedians','cmeans'):\n",
    "            if partname in parts:\n",
    "                parts[partname].set_edgecolor('black')\n",
    "                parts[partname].set_linewidth(1)\n",
    "\n",
    "        ax.set_xticks([i * 3 + 1.4 for i in range(len(epoch_indices))])\n",
    "        ax.set_xticklabels([str(i + 1) for i in epoch_indices])\n",
    "        ax.set_xlabel(f\"Epoch {xlabel_suffix}\", fontsize=11)\n",
    "        ax.set_ylabel(\"Activation Value\", fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        if combined:\n",
    "            flat = np.concatenate([np.ravel(arr) for arr in combined])\n",
    "            vmin, vmax = flat.min(), flat.max()\n",
    "            margin = (vmax - vmin) * 0.05 if vmax != vmin else 0.1\n",
    "            ax.set_ylim(vmin - margin, vmax + margin)\n",
    "\n",
    "    layer_names = [\"conv1\",\"conv2\",\"conv3\",\"conv4\",\"conv5\",\"lin1\",\"lin2\"]\n",
    "    num_epochs = len(epoch_activations1[layer_names[0]])\n",
    "    early_epochs = list(range(0, min(5, num_epochs)))\n",
    "    late_epochs = list(range(5, num_epochs))\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "\n",
    "    fig, axes = plt.subplots(len(layer_names), 2, figsize=(18, 28),\n",
    "                             gridspec_kw={'width_ratios': [8, 12]})\n",
    "    fig.suptitle(f\"Activation Distribution Comparison: {activation_name1} vs {activation_name2}\",\n",
    "                 fontsize=18, y=0.995)\n",
    "\n",
    "    for idx, layer_name in enumerate(layer_names):\n",
    "        ax_early = axes[idx, 0]\n",
    "        plot_violin_subplot(ax_early, epoch_activations1[layer_name],\n",
    "                           epoch_activations2[layer_name],\n",
    "                           early_epochs, \"(first 5)\", colors)\n",
    "        ax_early.set_title(f\"{layer_name} (first 5)\", fontsize=13, fontweight=\"bold\", pad=10)\n",
    "\n",
    "        ax_late = axes[idx, 1]\n",
    "        if late_epochs:\n",
    "            plot_violin_subplot(ax_late, epoch_activations1[layer_name],\n",
    "                               epoch_activations2[layer_name],\n",
    "                               late_epochs, \"(later)\", colors)\n",
    "            ax_late.set_title(f\"{layer_name} (later)\", fontsize=13, fontweight=\"bold\", pad=10)\n",
    "        else:\n",
    "            ax_late.axis('off')\n",
    "\n",
    "    legend_elements = [Patch(facecolor='lightblue', alpha=0.7, label=activation_name1),\n",
    "                       Patch(facecolor='lightcoral', alpha=0.7, label=activation_name2)]\n",
    "    fig.legend(handles=legend_elements, loc='upper right', fontsize=13, bbox_to_anchor=(0.98, 0.99))\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.995])\n",
    "    plt.savefig(f\"{save_dir}/figures/activation_violinplots_comparison_{activation_name1}_vs_{activation_name2}.png\",\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df57248",
   "metadata": {},
   "source": [
    "### 6. Train, Evaluate, and Visualize for models with small pixel value range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu=different_activation_CNN(activation=\"ReLU\").to(device)\n",
    "model_tanh=different_activation_CNN(activation=\"Tanh\").to(device)\n",
    "optimizer_relu=optim.SGD(model_relu.parameters(),lr=config[\"learning_rate\"],momentum=0.5)\n",
    "optimizer_tanh=optim.SGD(model_tanh.parameters(),lr=config[\"learning_rate\"],momentum=0.5)\n",
    "\n",
    "losses_relu,epoch_activations_relu=single_activation_train_with_recording(model_relu,train_loader,device,optimizer_relu)\n",
    "losses_tanh,epoch_activations_tanh=single_activation_train_with_recording(model_tanh,train_loader,device,optimizer_tanh)\n",
    "\n",
    "accuracy_relu=single_activation_test(model_relu,test_loader,device)\n",
    "accuracy_tanh=single_activation_test(model_tanh,test_loader,device)\n",
    "\n",
    "\n",
    "# Here is visualization\n",
    "plt.plot(losses_relu, \"-o\")\n",
    "plt.plot(losses_tanh, \"-o\")\n",
    "plt.title(\"Losses between ReLU and Tanh\")\n",
    "plt.legend([\"ReLU\",\"Tanh\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(linestyle='-', alpha=0.3)\n",
    "plt.savefig(f\"{save_dir}/figures/comparisons.png\")\n",
    "\n",
    "plot_activation_violinplots_comparison(epoch_activations_relu, 'ReLU', epoch_activations_tanh, 'Tanh', save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4120845",
   "metadata": {},
   "source": [
    "### 7. Train, Evaluate, and Visualize for models with large pixel value range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu_new=different_activation_CNN(activation=\"ReLU\").to(device)\n",
    "model_tanh_new=different_activation_CNN(activation=\"Tanh\").to(device)\n",
    "optimizer_relu_new=optim.SGD(model_relu_new.parameters(),lr=config[\"learning_rate\"],momentum=0.5)\n",
    "optimizer_tanh_new=optim.SGD(model_tanh_new.parameters(),lr=config[\"learning_rate\"],momentum=0.5)\n",
    "\n",
    "losses_relu_new,epoch_activations_relu_new=single_activation_train_with_recording(model_relu_new,train_loader_modified,device,optimizer_relu_new)\n",
    "losses_tanh_new,epoch_activations_tanh_new=single_activation_train_with_recording(model_tanh_new,train_loader_modified,device,optimizer_tanh_new)\n",
    "\n",
    "accuracy_relu_new=single_activation_test(model_relu_new,test_loader_modified,device)\n",
    "accuracy_tanh_new=single_activation_test(model_tanh_new,test_loader_modified,device)\n",
    "\n",
    "\n",
    "# Here is visualization\n",
    "plt.plot(losses_relu_new,'-o')\n",
    "plt.plot(losses_tanh_new,'-o')\n",
    "plt.title(\"Losses between ReLU and Tanh\")\n",
    "plt.legend([\"ReLU\",\"Tanh\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(linestyle='-', alpha=0.3)\n",
    "plt.savefig(f\"{save_dir}/figures/comparisons_new.png\")\n",
    "plt.show()\n",
    "\n",
    "plot_activation_violinplots_comparison(epoch_activations_relu_new, 'ReLU_new', epoch_activations_tanh_new, 'Tanh_new', save_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
