{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sgMy_e4Ro2k"
      },
      "source": [
        "# **Project Title:** Effects of Various Memory Infrastructure\n",
        "\n",
        "# **Conceptual Question**\n",
        "\n",
        "In this project, I will be exploring the RNN models. From the class I remembered professor teaching us about RNN's and telling how the hidden state is passed to the next hiddden layer to predict the next token. I understood the reason - to give the model more context about what token to predict - but I could not understand why passing this hidden layer is different than passing the previously predicted tokens directly like in n-gram nlp prediction.\n",
        "\n",
        "So I setup this experiment to find what is in the hidden state which is being passed in RNN and how the model prediction would change if I just pass the previous 'n' tokens? How effective are the other memory support infrastructures?\n",
        "\n",
        "\n",
        "# **Hypothesis**\n",
        "\n",
        "For simple short-range sequence prediction tasks, a standard RNN behaves similarly to a n-gram version of RNN. Both rely primarily on the most recent token, so providing the last token as explicit input can effectively compensate for removing the hidden state.\n",
        "\n",
        "As the task's required dependency range increases, the RNN's hidden state becomes increasingly advantageous compared to the n-gram model, because the hidden state can carry information from far earlier in the sequence in a fixed-size vector, while an n-gram model must explicitly expand its context window to match the performance.\n",
        "\n",
        "LSTM model, RNN with attention, and Bidirectional RNN would prove to be better than Vanilla RNN and N-gram model in both the tasks.\n",
        "\n",
        "# **Experimental Design**\n",
        "\n",
        "I will be performing this experiment over 2 tasks.\n",
        "\n",
        "Tasks\n",
        "\n",
        "We construct sequence prediction tasks where the model predicts a final token, which is a copy of an earlier token. This allows us to control and gradually increase the dependency range. The input is an incomplete palindromic sequence and the output should only be next token in the sequence.\n",
        "The length of the input varies between {5, 7, 9, 11, 13} and the forward part of the sequence is always more than half the length of the sequence, for example if total length is 5 then the sequence cannot be ABCBA (forward length = 3, backward length = 2). Here is a total list of sequence length and their corresponding allowed forward lengths:\n",
        "5: [4], 7: [4, 5], 9: [5, 6, 7], 11: [6, 7, 8, 9], 13: [7, 8, 9, 10, 11]. Each combination has equal weightage in the dataset.\n",
        "\n",
        "\n",
        "1. Task 1:\n",
        "The sequence can be started from any character but needs to follow alphabetical order. For ease, the letters are cyclical:\n",
        "A -> B -> C -> … -> Y -> Z -> A -> B -> …\n",
        "\n",
        "Examples:\n",
        "\n",
        "A B C D E D C → predict B\n",
        "\n",
        "F G H I H → predict G\n",
        "\n",
        "X Y Z A Z → predict Y\n",
        "\n",
        "K L M N O P Q → predict P\n",
        "\n",
        "\n",
        "3.  Task 2:\n",
        "The sequence can be started from any character and does not need to follow any alphabetical order. Forcing the model to remember the sequence instead of using the last token to predict the output.\n",
        "\n",
        "Examples:\n",
        "\n",
        "A J X D R D X → predict J\n",
        "\n",
        "F A C I C → predict A\n",
        "\n",
        "X S Z A Z → predict S\n",
        "\n",
        "K T A J I E P → predict E"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experimental Code**"
      ],
      "metadata": {
        "id": "yI2P5XRuR7Ou"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI_hCQMb80jJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KflLry2o8lD"
      },
      "source": [
        "# Creating Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXxHIYfdRjBq"
      },
      "outputs": [],
      "source": [
        "ALPHABET = list(string.ascii_uppercase)\n",
        "\n",
        "def generate_single_sequence(task, seq_len, forward_len, generated, offset=0) -> Tuple[List[str], str]:\n",
        "    alphabet = list(ALPHABET)\n",
        "\n",
        "    if task==2:\n",
        "      random.shuffle(alphabet)\n",
        "\n",
        "    start_idx = random.randint(0, len(alphabet) - 1) if task==2 else offset\n",
        "    seq = [alphabet[(start_idx + i) % len(alphabet)] for i in range(1, forward_len+1)]\n",
        "    seq += [alphabet[(start_idx + (forward_len - (i-forward_len))) % len(alphabet)] for i in range(forward_len+1, seq_len+1)]\n",
        "\n",
        "    target_token = seq[-1]\n",
        "    input_sequence = seq[:-1]\n",
        "\n",
        "    if (input_sequence, target_token) in generated: #avoid dataset pollution\n",
        "      return generate_single_sequence(task, seq_len, forward_len, generated, offset+1)\n",
        "\n",
        "    return input_sequence, target_token\n",
        "\n",
        "\n",
        "def generate_dataset(num_sequences, task, train_ratio = 0.8, seed = 42):\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "    lengths = {5: [4], 7: [4, 5], 9: [5, 6, 7], 11: [6, 7, 8, 9], 13: [7, 8, 9, 10, 11]}\n",
        "    train = []\n",
        "    test = []\n",
        "\n",
        "    if task==1:\n",
        "      for i in lengths:\n",
        "        for j in lengths[i]:\n",
        "          dataset = []\n",
        "          for k in range(26):\n",
        "            seq, target = generate_single_sequence(task, i, j, dataset, k)\n",
        "            dataset.append((seq, target))\n",
        "          split_idx = int(len(dataset) * train_ratio)\n",
        "          random.shuffle(dataset)\n",
        "          train += dataset[:split_idx]\n",
        "          test += dataset[split_idx:]\n",
        "    else:\n",
        "      for i in lengths:\n",
        "        per_sublength = int((0.2*num_sequences)//len(lengths[i]))\n",
        "        for j in lengths[i]:\n",
        "          dataset = []\n",
        "          for k in range(per_sublength):\n",
        "            seq, target = generate_single_sequence(task, i, j, dataset)\n",
        "            dataset.append((seq, target))\n",
        "          split_idx = int(len(dataset) * train_ratio)\n",
        "          random.shuffle(dataset)\n",
        "          train += dataset[:split_idx]\n",
        "          test += dataset[split_idx:]\n",
        "\n",
        "    random.shuffle(train)\n",
        "    random.shuffle(test)\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHFTlXqM-Xtv"
      },
      "source": [
        "# RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib1tl6Ke-b7h"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, vocab_size, num_layers=1):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.RNN(\n",
        "          input_size=input_size,\n",
        "          hidden_size=hidden_size,\n",
        "          num_layers=num_layers,\n",
        "          batch_first=True\n",
        "        )\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output, h_last = self.rnn(x)\n",
        "    logits = self.fc(output[:, -1, :])\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlH7kk9P-jaG"
      },
      "source": [
        "# N-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D50hyUc-rs0"
      },
      "outputs": [],
      "source": [
        "class Ngram(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_size, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "        self.fc1 = nn.Linear(input_size * (n-1), hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, in_dim = x.shape\n",
        "        if seq_len < self.n:\n",
        "            pad_len = self.n - seq_len\n",
        "            pad = x.new_zeros(batch_size, pad_len, in_dim)\n",
        "            x_padded = torch.cat([pad, x], dim=1)\n",
        "            window = x_padded[:, -self.n+1:, :]\n",
        "        else:\n",
        "            window = x[:, -self.n+1:, :]\n",
        "\n",
        "        window_flat = window.reshape(batch_size, -1)\n",
        "\n",
        "        h = torch.tanh(self.fc1(window_flat))\n",
        "        logits = self.fc2(h)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "OdwhJlmEuDjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, vocab_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(\n",
        "          input_size=input_size,\n",
        "          hidden_size=hidden_size,\n",
        "          num_layers=num_layers,\n",
        "          batch_first=True\n",
        "        )\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output, (h_last, c_last) = self.lstm(x)\n",
        "    logits = self.fc(output[:, -1, :])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "q7d4mVDFuAui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN with attention"
      ],
      "metadata": {
        "id": "iPC_P0fdvyK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, attn_dim, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.Wq = nn.Linear(input_size, attn_dim)\n",
        "        self.Wk = nn.Linear(input_size, attn_dim)\n",
        "        self.Wv = nn.Linear(input_size, attn_dim)\n",
        "\n",
        "        self.rnn_cell = nn.RNNCell(input_size + attn_dim, hidden_size)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X.squeeze(0)\n",
        "        total_length = X.shape[0]\n",
        "\n",
        "        h = X.new_zeros(self.rnn_cell.hidden_size)\n",
        "\n",
        "        K = self.Wk(X)\n",
        "        V = self.Wv(X)\n",
        "\n",
        "        for i in range(total_length):\n",
        "            q = self.Wq(X[i])\n",
        "            r = q @ K.T\n",
        "            a = F.softmax(r, dim = 0)\n",
        "            c = a @ V\n",
        "            inp = torch.cat([X[i], c])\n",
        "            h = self.rnn_cell(inp, h)\n",
        "        output = self.fc(h).unsqueeze(0)\n",
        "        return output"
      ],
      "metadata": {
        "id": "J81lV2Amv12n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional RNN"
      ],
      "metadata": {
        "id": "f7WOHF9aNXBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, vocab_size, num_layers=1):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.RNN(\n",
        "          input_size=input_size,\n",
        "          hidden_size=hidden_size,\n",
        "          num_layers=num_layers,\n",
        "          batch_first=True,\n",
        "          bidirectional=True\n",
        "        )\n",
        "    self.fc = nn.Linear(2*hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output, h_last = self.rnn(x)\n",
        "    logits = self.fc(output[:, -1, :])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "xWpwoTToNaFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnhHzVoG-uer"
      },
      "source": [
        "# Supporting methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX17BQaqVqvv"
      },
      "outputs": [],
      "source": [
        "def character_to_one_hot(c):\n",
        "  onehot = torch.zeros(26)\n",
        "  onehot[(ord(c) - ord('A'))] = 1\n",
        "  return onehot\n",
        "\n",
        "def seq_to_one_hot(seq):\n",
        "  onehot = torch.zeros(len(seq), 26)\n",
        "  for i, c in enumerate(seq):\n",
        "    onehot[i] = character_to_one_hot(c)\n",
        "  return onehot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDOOtmEU_wng"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, dataset, epochs, lr = 1e-3, clip_grad = 1.0):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x_data, y_data in dataset:\n",
        "            x = seq_to_one_hot(x_data).unsqueeze(0)\n",
        "            y = torch.tensor([ord(y_data) - ord(\"A\")])\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            if clip_grad is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataset)\n",
        "        print(f\"Epoch {epoch + 1}: Loss {avg_loss:.4f}\")\n",
        "\n",
        "def evaluate(model: nn.Module, dataset) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x_data, y_data in dataset:\n",
        "            x = seq_to_one_hot(x_data).unsqueeze(0)\n",
        "            y_idx = ord(y_data) - ord(\"A\")\n",
        "\n",
        "            logits = model(x)\n",
        "            pred = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "            if pred == y_idx:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return 100.0 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMp6On6grSB6"
      },
      "source": [
        "# Task 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = generate_dataset(1000, task = 1)\n",
        "print(\"Running the Shorter Task\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzz8wRaSKlTv",
        "outputId": "0c643012-9bab-40f0-ea73-b979260c8b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Shorter Task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZL7ihpdaZT1",
        "outputId": "352b4b54-e511-4c11-aa19-5d79259e3cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running N-gram model with n=3\n",
            "Epoch 1: Loss 2.5022\n",
            "Epoch 2: Loss 0.5985\n",
            "Epoch 3: Loss 0.0725\n",
            "Epoch 4: Loss 0.0203\n",
            "Epoch 5: Loss 0.0101\n",
            "Epoch 6: Loss 0.0061\n",
            "Epoch 7: Loss 0.0040\n",
            "Epoch 8: Loss 0.0028\n",
            "Epoch 9: Loss 0.0020\n",
            "Epoch 10: Loss 0.0015\n",
            "Trigram MLP accuracy:  100.0 %\n",
            "\n",
            "Running RNN model\n",
            "Epoch 1: Loss 2.4103\n",
            "Epoch 2: Loss 0.8771\n",
            "Epoch 3: Loss 0.0812\n",
            "Epoch 4: Loss 0.0083\n",
            "Epoch 5: Loss 0.0037\n",
            "Epoch 6: Loss 0.0021\n",
            "Epoch 7: Loss 0.0014\n",
            "Epoch 8: Loss 0.0009\n",
            "Epoch 9: Loss 0.0007\n",
            "Epoch 10: Loss 0.0005\n",
            "RNN accuracy:  100.0 %\n",
            "\n",
            "Running LSTM model\n",
            "Epoch 1: Loss 2.9964\n",
            "Epoch 2: Loss 2.4417\n",
            "Epoch 3: Loss 2.0037\n",
            "Epoch 4: Loss 1.3346\n",
            "Epoch 5: Loss 0.6463\n",
            "Epoch 6: Loss 0.2526\n",
            "Epoch 7: Loss 0.1103\n",
            "Epoch 8: Loss 0.0511\n",
            "Epoch 9: Loss 0.0233\n",
            "Epoch 10: Loss 0.0125\n",
            "LSTM accuracy:  100.0 %\n",
            "\n",
            "Running RNN w/ attention model\n",
            "Epoch 1: Loss 2.5775\n",
            "Epoch 2: Loss 1.6964\n",
            "Epoch 3: Loss 0.7870\n",
            "Epoch 4: Loss 0.1708\n",
            "Epoch 5: Loss 0.0298\n",
            "Epoch 6: Loss 0.0062\n",
            "Epoch 7: Loss 0.0029\n",
            "Epoch 8: Loss 0.0017\n",
            "Epoch 9: Loss 0.0012\n",
            "Epoch 10: Loss 0.0009\n",
            "RNN w/ attention accuracy:  100.0 %\n",
            "\n",
            "Running Bidirectional RNN\n",
            "Epoch 1: Loss 2.3638\n",
            "Epoch 2: Loss 0.9486\n",
            "Epoch 3: Loss 0.0997\n",
            "Epoch 4: Loss 0.0090\n",
            "Epoch 5: Loss 0.0036\n",
            "Epoch 6: Loss 0.0021\n",
            "Epoch 7: Loss 0.0013\n",
            "Epoch 8: Loss 0.0009\n",
            "Epoch 9: Loss 0.0007\n",
            "Epoch 10: Loss 0.0005\n",
            "RNN w/ attention accuracy:  100.0 %\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nRunning N-gram model with n=3\")\n",
        "ngram = Ngram(input_size=26, hidden_size=128, vocab_size=26, n=3)\n",
        "train(ngram, train_loader, epochs = 10)\n",
        "accuracy_ngram3 = evaluate(ngram, test_loader)\n",
        "print(\"Trigram MLP accuracy: \", accuracy_ngram3, \"%\")\n",
        "\n",
        "print(\"\\nRunning RNN model\")\n",
        "rnn = RNN(input_size=26, hidden_size=128, vocab_size=26)\n",
        "train(rnn, train_loader, epochs = 10)\n",
        "accuracy_rnn = evaluate(rnn, test_loader)\n",
        "print(\"RNN accuracy: \", accuracy_rnn,\"%\")\n",
        "\n",
        "print(\"\\nRunning LSTM model\")\n",
        "lstm = LSTM(input_size=26, hidden_size=128, vocab_size=26)\n",
        "train(lstm, train_loader, epochs = 10)\n",
        "accuracy_lstm = evaluate(lstm, test_loader)\n",
        "print(\"LSTM accuracy: \", accuracy_lstm,\"%\")\n",
        "\n",
        "print(\"\\nRunning RNN w/ attention model\")\n",
        "rnn_attention = RNNAttention(input_size=26, hidden_size=128, attn_dim=64, vocab_size = 26)\n",
        "train(rnn_attention, train_loader, epochs = 10)\n",
        "accuracy_rnn = evaluate(rnn_attention, test_loader)\n",
        "print(\"RNN w/ attention accuracy: \", accuracy_rnn,\"%\")\n",
        "\n",
        "print(\"\\nRunning Bidirectional RNN\")\n",
        "rnn_bidirectional = BidirectionalRNN(input_size=26, hidden_size=128, vocab_size = 26)\n",
        "train(rnn_bidirectional, train_loader, epochs = 10)\n",
        "accuracy_rnn_bi = evaluate(rnn_bidirectional, test_loader)\n",
        "print(\"RNN w/ attention accuracy: \", accuracy_rnn_bi,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrKEWE2oghF"
      },
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g4iuEYSoJqm",
        "outputId": "b0467a2a-fc05-48c5-803a-ff5279432117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Long Task\n"
          ]
        }
      ],
      "source": [
        "train_loader, test_loader = generate_dataset(5000, task = 2)\n",
        "print(\"Running the Long Task\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning N-gram model with n=3\")\n",
        "ngram = Ngram(input_size=26, hidden_size=128, vocab_size=26, n=3)\n",
        "train(ngram, train_loader, epochs = 20)\n",
        "accuracy_ngram3 = evaluate(ngram, test_loader)\n",
        "print(\"Trigram MLP accuracy: \", accuracy_ngram3, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBJbRaT1SsNN",
        "outputId": "cbcd332c-3ac9-4d0f-aa4a-4b5514ecc24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running N-gram model with n=3\n",
            "Epoch 1: Loss 3.1721\n",
            "Epoch 2: Loss 3.0540\n",
            "Epoch 3: Loss 3.0415\n",
            "Epoch 4: Loss 3.0368\n",
            "Epoch 5: Loss 3.0342\n",
            "Epoch 6: Loss 3.0325\n",
            "Epoch 7: Loss 3.0312\n",
            "Epoch 8: Loss 3.0301\n",
            "Epoch 9: Loss 3.0290\n",
            "Epoch 10: Loss 3.0280\n",
            "Epoch 11: Loss 3.0269\n",
            "Epoch 12: Loss 3.0258\n",
            "Epoch 13: Loss 3.0244\n",
            "Epoch 14: Loss 3.0229\n",
            "Epoch 15: Loss 3.0211\n",
            "Epoch 16: Loss 3.0191\n",
            "Epoch 17: Loss 3.0168\n",
            "Epoch 18: Loss 3.0143\n",
            "Epoch 19: Loss 3.0115\n",
            "Epoch 20: Loss 3.0090\n",
            "Trigram MLP accuracy:  18.98101898101898 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning RNN model\")\n",
        "rnn = RNN(input_size=26, hidden_size=128, vocab_size=26)\n",
        "train(rnn, train_loader, epochs = 20)\n",
        "accuracy_rnn = evaluate(rnn, test_loader)\n",
        "print(\"RNN accuracy: \", accuracy_rnn,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt0tLulMSuDh",
        "outputId": "e417f385-2ba1-47ba-c6d5-4fba4b4c1f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running RNN model\n",
            "Epoch 1: Loss 3.2295\n",
            "Epoch 2: Loss 3.0381\n",
            "Epoch 3: Loss 2.9264\n",
            "Epoch 4: Loss 2.8262\n",
            "Epoch 5: Loss 2.7760\n",
            "Epoch 6: Loss 2.7281\n",
            "Epoch 7: Loss 2.6857\n",
            "Epoch 8: Loss 2.7020\n",
            "Epoch 9: Loss 2.6646\n",
            "Epoch 10: Loss 2.6429\n",
            "Epoch 11: Loss 2.6333\n",
            "Epoch 12: Loss 2.6013\n",
            "Epoch 13: Loss 2.5973\n",
            "Epoch 14: Loss 2.6067\n",
            "Epoch 15: Loss 2.5339\n",
            "Epoch 16: Loss 2.5692\n",
            "Epoch 17: Loss 2.5379\n",
            "Epoch 18: Loss 2.5124\n",
            "Epoch 19: Loss 2.4944\n",
            "Epoch 20: Loss 2.4951\n",
            "RNN accuracy:  29.47052947052947 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning RNN w/ attention model\")\n",
        "rnn_attention = RNNAttention(input_size=26, hidden_size=128, attn_dim=64, vocab_size = 26)\n",
        "train(rnn_attention, train_loader, epochs = 10)\n",
        "accuracy_rnn_att = evaluate(rnn_attention, test_loader)\n",
        "print(\"RNN w/ attention accuracy: \", accuracy_rnn_att,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaD0ozkKSwzB",
        "outputId": "bb104be9-2ca5-4d25-f7d9-3181bd061dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running RNN w/ attention model\n",
            "Epoch 1: Loss 3.0914\n",
            "Epoch 2: Loss 2.7174\n",
            "Epoch 3: Loss 2.5607\n",
            "Epoch 4: Loss 2.4420\n",
            "Epoch 5: Loss 2.3520\n",
            "Epoch 6: Loss 2.2481\n",
            "Epoch 7: Loss 2.1375\n",
            "Epoch 8: Loss 2.0818\n",
            "Epoch 9: Loss 2.0586\n",
            "Epoch 10: Loss 2.0984\n",
            "RNN w/ attention accuracy:  23.076923076923077 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning LSTM model\")\n",
        "lstm = LSTM(input_size=26, hidden_size=128, vocab_size=26)\n",
        "train(lstm, train_loader, epochs = 20)\n",
        "accuracy_lstm = evaluate(lstm, test_loader)\n",
        "print(\"LSTM accuracy: \", accuracy_lstm,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDPkgPDMS6GW",
        "outputId": "6f1bd5b4-69b4-4d93-a025-25eed517d13f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running LSTM model\n",
            "Epoch 1: Loss 3.2325\n",
            "Epoch 2: Loss 3.1068\n",
            "Epoch 3: Loss 2.9693\n",
            "Epoch 4: Loss 2.6998\n",
            "Epoch 5: Loss 2.3271\n",
            "Epoch 6: Loss 1.9817\n",
            "Epoch 7: Loss 1.6979\n",
            "Epoch 8: Loss 1.4300\n",
            "Epoch 9: Loss 1.1703\n",
            "Epoch 10: Loss 0.9368\n",
            "Epoch 11: Loss 0.7345\n",
            "Epoch 12: Loss 0.5590\n",
            "Epoch 13: Loss 0.4328\n",
            "Epoch 14: Loss 0.3440\n",
            "Epoch 15: Loss 0.2502\n",
            "Epoch 16: Loss 0.2262\n",
            "Epoch 17: Loss 0.1979\n",
            "Epoch 18: Loss 0.1693\n",
            "Epoch 19: Loss 0.1724\n",
            "Epoch 20: Loss 0.1547\n",
            "LSTM accuracy:  37.46253746253746 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning Bidirectional RNN\")\n",
        "rnn_bi = BidirectionalRNN(input_size=26, hidden_size=128, vocab_size = 26)\n",
        "train(rnn_bi, train_loader, epochs = 20)\n",
        "accuracy_rnn_bi = evaluate(rnn_bi, test_loader)\n",
        "print(\"RNN w/ attention accuracy: \", accuracy_rnn_bi,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qi6zt8HPdKq",
        "outputId": "8e93b83d-8bc2-4ce1-8aa8-20945ea884f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Bidirectional RNN\n",
            "Epoch 1: Loss 3.2345\n",
            "Epoch 2: Loss 3.0516\n",
            "Epoch 3: Loss 2.8986\n",
            "Epoch 4: Loss 2.8221\n",
            "Epoch 5: Loss 2.7519\n",
            "Epoch 6: Loss 2.7085\n",
            "Epoch 7: Loss 2.6945\n",
            "Epoch 8: Loss 2.6941\n",
            "Epoch 9: Loss 2.6575\n",
            "Epoch 10: Loss 2.6581\n",
            "Epoch 11: Loss 2.6031\n",
            "Epoch 12: Loss 2.5817\n",
            "Epoch 13: Loss 2.5650\n",
            "Epoch 14: Loss 2.5466\n",
            "Epoch 15: Loss 2.5485\n",
            "Epoch 16: Loss 2.5051\n",
            "Epoch 17: Loss 2.5079\n",
            "Epoch 18: Loss 2.4921\n",
            "Epoch 19: Loss 2.4944\n",
            "Epoch 20: Loss 2.4902\n",
            "RNN w/ attention accuracy:  29.37062937062937 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}